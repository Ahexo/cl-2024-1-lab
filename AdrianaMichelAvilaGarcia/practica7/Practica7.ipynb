{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a65613b-6549-4c2d-9657-8fcbd2acf7c1",
   "metadata": {},
   "source": [
    "# Práctica 7: Modelos del lenguaje"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c893ad-265d-42c7-8812-b7bc42fe69d8",
   "metadata": {},
   "source": [
    "## Preprocesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd631c91-d100-4baf-833c-82581bc2062d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Leer archivo y generar lista de oraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b22f8a05-e70b-4154-896a-771d29901f1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = './el-quijote.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c6faff8f-774d-4cfd-89a1-b3dbf40315d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35522"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "encoding = 'utf-8-sig'\n",
    "\n",
    "try:\n",
    "    with open(path, 'r', encoding=encoding) as f:\n",
    "        contents = f.readlines()\n",
    "        #parsed_corpus = [ ast.literal_eval(x) for x in contents ]\n",
    "except UnicodeDecodeError:\n",
    "    print(\"Error: Unable to decode the file with the specified encoding.\")\n",
    "    \n",
    "len(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4e3450ef-2569-484f-8102-12b0af15d872",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['podadera. Frisaba la edad de nuestro hidalgo con los cincuenta años; era de\\n',\n",
       " 'complexión recia, seco de carnes, enjuto de rostro, gran madrugador y amigo\\n',\n",
       " 'de la caza. Quieren decir que tenía el sobrenombre de Quijada, o Quesada,\\n',\n",
       " 'que en esto hay alguna diferencia en los autores que deste caso escriben;\\n',\n",
       " 'aunque, por conjeturas verosímiles, se deja entender que se llamaba\\n',\n",
       " 'Quejana. Pero esto importa poco a nuestro cuento; basta que en la narración\\n',\n",
       " 'dél no se salga un punto de la verdad.\\n',\n",
       " '\\n',\n",
       " 'Es, pues, de saber que este sobredicho hidalgo, los ratos que estaba\\n',\n",
       " 'ocioso, que eran los más del año, se daba a leer libros de caballerías, con\\n']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "839201be-96c0-43ee-92c3-f641296ac25c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import operator\n",
    "\n",
    "def get_sentences(corpus):\n",
    "    \"\"\"\n",
    "    Recibe una lista de líneas que terminan en '\\n', y genera otra lista que contiene el mismo\n",
    "    texto, pero con '\\n\\n' y '.' como separador.\n",
    "    También se deshace de todos los '...'\n",
    "    \"\"\"\n",
    "    parsed_corpus = [ '<linejump>' if x == '\\n' else x[:-1] for x in contents ]\n",
    "    parsed_corpus = ' '.join(parsed_corpus).split('<linejump>')\n",
    "    parsed_corpus = [x.strip() for x in parsed_corpus]\n",
    "    parsed_corpus = [ x.replace('...','').replace('.', '.<eos>').split('<eos>') for x in parsed_corpus ]\n",
    "    parsed_corpus = reduce(operator.concat, parsed_corpus)\n",
    "    parsed_corpus = filter(lambda x: len(x)>0, parsed_corpus)\n",
    "    parsed_corpus = [x.strip() for x in parsed_corpus]\n",
    "    return parsed_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e06fd392-a7f3-4024-a33c-e4fde237dfa7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['En un lugar de la Mancha, de cuyo nombre no quiero acordarme, no ha mucho tiempo que vivía un hidalgo de los de lanza en astillero, adarga antigua, rocín flaco y galgo corredor.',\n",
       " 'Una olla de algo más vaca que carnero, salpicón las más noches, duelos y quebrantos los sábados, lantejas los viernes, algún palomino de añadidura los domingos, consumían las tres partes de su hacienda.',\n",
       " 'El resto della concluían sayo de velarte, calzas de velludo para las fiestas, con sus pantuflos de lo mesmo, y los días de entresemana se honraba con su vellorí de lo más fino.',\n",
       " 'Tenía en su casa una ama que pasaba de los cuarenta, y una sobrina que no llegaba a los veinte, y un mozo de campo y plaza, que así ensillaba el rocín como tomaba la podadera.',\n",
       " 'Frisaba la edad de nuestro hidalgo con los cincuenta años; era de complexión recia, seco de carnes, enjuto de rostro, gran madrugador y amigo de la caza.',\n",
       " 'Quieren decir que tenía el sobrenombre de Quijada, o Quesada, que en esto hay alguna diferencia en los autores que deste caso escriben; aunque, por conjeturas verosímiles, se deja entender que se llamaba Quejana.',\n",
       " 'Pero esto importa poco a nuestro cuento; basta que en la narración dél no se salga un punto de la verdad.',\n",
       " 'Es, pues, de saber que este sobredicho hidalgo, los ratos que estaba ocioso, que eran los más del año, se daba a leer libros de caballerías, con tanta afición y gusto, que olvidó casi de todo punto el ejercicio de la caza, y aun la administración de su hacienda.',\n",
       " 'Y llegó a tanto su curiosidad y desatino en esto, que vendió muchas hanegas de tierra de sembradura para comprar libros de caballerías en que leer, y así, llevó a su casa todos cuantos pudo haber dellos; y de todos, ningunos le parecían tan bien como los que compuso el famoso Feliciano de Silva, porque la claridad de su prosa y aquellas entricadas razones suyas le parecían de perlas, y más cuando llegaba a leer aquellos requiebros y cartas de desafíos, donde en muchas partes hallaba escrito: La razón de la sinrazón que a mi razón se hace, de tal manera mi razón enflaquece, que con razón me quejo de la vuestra fermosura.',\n",
       " 'Y también cuando leía: los altos cielos que de vuestra divinidad divinamente con las estrellas os fortifican, y os hacen merecedora del merecimiento que merece la vuestra grandeza.']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_sentences = get_sentences(contents)\n",
    "corpus_sentences[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649b0190-4533-4aa8-9ca1-766e78bc44ec",
   "metadata": {},
   "source": [
    "### Preprocesar oraciones\n",
    "\n",
    "Pasar todo a minúsculas, y eliminar algunos signos de puntuación (guión largo, comillas, y comillas españolas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "947f5af3-0830-4c78-98bc-aca71a40db55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def delete_substrings(string: str, chars: list) -> str:\n",
    "    final_str = string\n",
    "    for char in chars:\n",
    "        final_str = final_str.replace(char, '')\n",
    "    return final_str\n",
    "\n",
    "def process_sentences(sentences: list[str], stopwords: list) -> list:\n",
    "    clean_sentences = [delete_substrings(sent.lower(), stopwords) for sent in sentences]\n",
    "    # Para que la tokenización funcione necesitamos que los signos de puntuación estén separadas de las palabras.\n",
    "    # Es decir, algo como \"palabra,\" debe estar como \"palabra ,\"\n",
    "    # Haremos esto con el tokenizer de nltk\n",
    "    processed_sentences = [' '.join(word_tokenize(t)) for t in clean_sentences]\n",
    "    return processed_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "88679d46-8927-4ba8-b09d-c4b43b4635ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stopwords = [\"''\",\"—\",\"«\",\"»\"]\n",
    "corpus_sentences = process_sentences(corpus_sentences, stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "39c3c5b3-b104-4e03-8585-2d3a8743172e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['en un lugar de la mancha , de cuyo nombre no quiero acordarme , no ha mucho tiempo que vivía un hidalgo de los de lanza en astillero , adarga antigua , rocín flaco y galgo corredor .',\n",
       " 'una olla de algo más vaca que carnero , salpicón las más noches , duelos y quebrantos los sábados , lantejas los viernes , algún palomino de añadidura los domingos , consumían las tres partes de su hacienda .',\n",
       " 'el resto della concluían sayo de velarte , calzas de velludo para las fiestas , con sus pantuflos de lo mesmo , y los días de entresemana se honraba con su vellorí de lo más fino .',\n",
       " 'tenía en su casa una ama que pasaba de los cuarenta , y una sobrina que no llegaba a los veinte , y un mozo de campo y plaza , que así ensillaba el rocín como tomaba la podadera .',\n",
       " 'frisaba la edad de nuestro hidalgo con los cincuenta años ; era de complexión recia , seco de carnes , enjuto de rostro , gran madrugador y amigo de la caza .',\n",
       " 'quieren decir que tenía el sobrenombre de quijada , o quesada , que en esto hay alguna diferencia en los autores que deste caso escriben ; aunque , por conjeturas verosímiles , se deja entender que se llamaba quejana .',\n",
       " 'pero esto importa poco a nuestro cuento ; basta que en la narración dél no se salga un punto de la verdad .',\n",
       " 'es , pues , de saber que este sobredicho hidalgo , los ratos que estaba ocioso , que eran los más del año , se daba a leer libros de caballerías , con tanta afición y gusto , que olvidó casi de todo punto el ejercicio de la caza , y aun la administración de su hacienda .',\n",
       " 'y llegó a tanto su curiosidad y desatino en esto , que vendió muchas hanegas de tierra de sembradura para comprar libros de caballerías en que leer , y así , llevó a su casa todos cuantos pudo haber dellos ; y de todos , ningunos le parecían tan bien como los que compuso el famoso feliciano de silva , porque la claridad de su prosa y aquellas entricadas razones suyas le parecían de perlas , y más cuando llegaba a leer aquellos requiebros y cartas de desafíos , donde en muchas partes hallaba escrito : la razón de la sinrazón que a mi razón se hace , de tal manera mi razón enflaquece , que con razón me quejo de la vuestra fermosura .',\n",
       " 'y también cuando leía : los altos cielos que de vuestra divinidad divinamente con las estrellas os fortifican , y os hacen merecedora del merecimiento que merece la vuestra grandeza .']"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_sentences[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e533bc42-c6ac-4a30-a6c2-c721bfccfa26",
   "metadata": {},
   "source": [
    "### Tokenizar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd12c23-d289-4b87-83e8-f23f7af716c8",
   "metadata": {},
   "source": [
    "Vamos a usar subword tokenization, como se muestra en el notebook de la práctica 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6f507eb2-0cc0-4531-a974-ca12399b015a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting subword-nmt\n",
      "  Downloading subword_nmt-0.3.8-py3-none-any.whl (27 kB)\n",
      "Collecting mock (from subword-nmt)\n",
      "  Obtaining dependency information for mock from https://files.pythonhosted.org/packages/6b/20/471f41173930550f279ccb65596a5ac19b9ac974a8d93679bcd3e0c31498/mock-5.1.0-py3-none-any.whl.metadata\n",
      "  Downloading mock-5.1.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\dell\\miniconda3\\envs\\nlp-environment\\lib\\site-packages (from subword-nmt) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\miniconda3\\envs\\nlp-environment\\lib\\site-packages (from tqdm->subword-nmt) (0.4.6)\n",
      "Downloading mock-5.1.0-py3-none-any.whl (30 kB)\n",
      "Installing collected packages: mock, subword-nmt\n",
      "Successfully installed mock-5.1.0 subword-nmt-0.3.8\n"
     ]
    }
   ],
   "source": [
    "!pip install subword-nmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "8a0f8645-6b68-48f2-88e0-a2a428b0c8d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Entrenamos un tokenizador con BPE usando subword-nmt.\n",
    "# Para esto necesitamos ingresarle el texto plano del corpus:\n",
    "\n",
    "def write_plain_text_corpus(raw_text: str, file_name: str) -> None:\n",
    "    with open(f\"{file_name}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(raw_text)\n",
    "\n",
    "plain_corpus = '\\n'.join(corpus_sentences)\n",
    "write_plain_text_corpus(plain_corpus, \"plain_processed_quijote\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "755cf061-8e80-4545-887f-a21a4a6daed4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: 425913\n",
      "types in corpus: 22653\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Primero checamos de qué tamaño es nuestro vocabulario\n",
    "print(\"tokens:\", len(plain_corpus.split()))\n",
    "types_in_corpus = Counter(plain_corpus.split())\n",
    "print(\"types in corpus:\", len(types_in_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "35392471-9d5a-4503-b15e-5f9a7a1bfe38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 39209),\n",
       " ('que', 20079),\n",
       " ('y', 17688),\n",
       " ('de', 17540),\n",
       " ('la', 10003),\n",
       " ('a', 9565),\n",
       " ('en', 7967),\n",
       " ('el', 7933),\n",
       " ('.', 7715),\n",
       " ('no', 6126),\n",
       " (';', 4709),\n",
       " ('los', 4630),\n",
       " ('se', 4549),\n",
       " ('con', 4053),\n",
       " ('por', 3785),\n",
       " ('las', 3381),\n",
       " ('lo', 3347),\n",
       " ('le', 3331),\n",
       " ('su', 3273),\n",
       " ('don', 2547),\n",
       " ('del', 2370),\n",
       " ('me', 2298),\n",
       " ('como', 2215),\n",
       " ('sancho', 2118),\n",
       " ('quijote', 2078),\n",
       " ('es', 2074),\n",
       " ('yo', 2041),\n",
       " (':', 2009),\n",
       " ('más', 1983),\n",
       " ('si', 1914),\n",
       " ('un', 1885),\n",
       " ('dijo', 1801),\n",
       " ('mi', 1671),\n",
       " ('al', 1669),\n",
       " ('para', 1393),\n",
       " ('porque', 1377),\n",
       " ('ni', 1328),\n",
       " ('una', 1308),\n",
       " ('él', 1258),\n",
       " ('tan', 1206)]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "types_in_corpus.most_common(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "6461f42d-bc09-45ed-bb2f-dad7fa31b636",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]\n",
      "  0%|          | 5/2000 [00:00<00:57, 34.42it/s]\n",
      "  0%|          | 9/2000 [00:00<00:53, 37.04it/s]\n",
      "  1%|          | 14/2000 [00:00<00:49, 40.03it/s]\n",
      "  1%|1         | 21/2000 [00:00<00:40, 49.35it/s]\n",
      "  1%|1         | 27/2000 [00:00<00:37, 52.75it/s]\n",
      "  2%|1         | 35/2000 [00:00<00:32, 61.12it/s]\n",
      "  2%|2         | 43/2000 [00:00<00:29, 65.75it/s]\n",
      "  3%|2         | 54/2000 [00:00<00:24, 78.51it/s]\n",
      "  3%|3         | 68/2000 [00:01<00:22, 87.70it/s]\n",
      "  4%|4         | 84/2000 [00:01<00:18, 105.87it/s]\n",
      "  6%|5         | 111/2000 [00:01<00:12, 149.43it/s]\n",
      "  7%|6         | 134/2000 [00:01<00:10, 171.46it/s]\n",
      "  8%|7         | 158/2000 [00:01<00:09, 187.59it/s]\n",
      "  9%|9         | 186/2000 [00:01<00:08, 209.88it/s]\n",
      " 11%|#         | 211/2000 [00:01<00:08, 219.30it/s]\n",
      " 12%|#1        | 238/2000 [00:01<00:07, 230.24it/s]\n",
      " 13%|#3        | 266/2000 [00:01<00:07, 241.28it/s]\n",
      " 15%|#4        | 291/2000 [00:01<00:07, 242.08it/s]\n",
      " 16%|#6        | 322/2000 [00:02<00:06, 261.78it/s]\n",
      " 18%|#7        | 354/2000 [00:02<00:05, 278.04it/s]\n",
      " 19%|#9        | 384/2000 [00:02<00:05, 280.51it/s]\n",
      " 21%|##        | 415/2000 [00:02<00:05, 285.71it/s]\n",
      " 23%|##2       | 454/2000 [00:02<00:04, 312.08it/s]\n",
      " 24%|##4       | 488/2000 [00:02<00:04, 315.55it/s]\n",
      " 26%|##6       | 521/2000 [00:02<00:04, 313.87it/s]\n",
      " 28%|##7       | 553/2000 [00:02<00:04, 309.20it/s]\n",
      " 29%|##9       | 589/2000 [00:02<00:04, 319.12it/s]\n",
      " 31%|###1      | 625/2000 [00:02<00:04, 330.63it/s]\n",
      " 33%|###3      | 666/2000 [00:03<00:03, 347.92it/s]\n",
      " 35%|###5      | 702/2000 [00:03<00:03, 346.26it/s]\n",
      " 38%|###7      | 754/2000 [00:03<00:03, 391.79it/s]\n",
      " 40%|###9      | 794/2000 [00:03<00:03, 392.88it/s]\n",
      " 42%|####1     | 839/2000 [00:03<00:02, 407.02it/s]\n",
      " 44%|####4     | 884/2000 [00:03<00:02, 414.25it/s]\n",
      " 46%|####6     | 926/2000 [00:03<00:02, 396.32it/s]\n",
      " 49%|####8     | 972/2000 [00:03<00:02, 411.48it/s]\n",
      " 51%|#####     | 1017/2000 [00:03<00:02, 417.76it/s]\n",
      " 53%|#####3    | 1064/2000 [00:04<00:02, 427.91it/s]\n",
      " 55%|#####5    | 1107/2000 [00:04<00:02, 423.22it/s]\n",
      " 58%|#####8    | 1160/2000 [00:04<00:01, 452.08it/s]\n",
      " 60%|######    | 1209/2000 [00:04<00:01, 457.07it/s]\n",
      " 63%|######3   | 1263/2000 [00:04<00:01, 475.72it/s]\n",
      " 66%|######5   | 1311/2000 [00:04<00:01, 471.47it/s]\n",
      " 68%|######8   | 1363/2000 [00:04<00:01, 483.00it/s]\n",
      " 71%|#######   | 1416/2000 [00:04<00:01, 486.03it/s]\n",
      " 74%|#######3  | 1473/2000 [00:04<00:01, 502.93it/s]\n",
      " 76%|#######6  | 1529/2000 [00:04<00:00, 512.98it/s]\n",
      " 79%|#######9  | 1585/2000 [00:05<00:00, 516.97it/s]\n",
      " 82%|########2 | 1646/2000 [00:05<00:00, 541.60it/s]\n",
      " 85%|########5 | 1706/2000 [00:05<00:00, 551.75it/s]\n",
      " 89%|########8 | 1775/2000 [00:05<00:00, 582.30it/s]\n",
      " 92%|#########2| 1845/2000 [00:05<00:00, 608.36it/s]\n",
      " 96%|#########5| 1913/2000 [00:05<00:00, 621.07it/s]\n",
      " 99%|#########9| 1981/2000 [00:05<00:00, 626.94it/s]\n",
      "100%|##########| 2000/2000 [00:05<00:00, 348.05it/s]\n"
     ]
    }
   ],
   "source": [
    "!subword-nmt learn-bpe -s 2000 < plain_processed_quijote.txt > quijote_tokenizer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "c84f96d4-8f3c-4ab6-bd3d-b09589d02c90",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized: @@ camin@@ ando nuestro fl@@ am@@ ante aventu@@ r@@ er@@ o@@  \n"
     ]
    }
   ],
   "source": [
    "tokenized = !echo \"caminando nuestro flamante aventurero\" | subword-nmt apply-bpe -c quijote_tokenizer.model\n",
    "print(\"tokenized:\", list(tokenized)[0].replace('\"',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "283f07e0-96e9-401f-9a37-2ec05573f159",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!subword-nmt apply-bpe -c quijote_tokenizer.model < plain_processed_quijote.txt > quijote_tokenized_2k.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "087a3bfc-f64c-45bc-8d47-42978fe6ef6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ahora, con esta tokenización, veremos cuántos tokens y tipos hay\n",
    "\n",
    "with open(\"quijote_tokenized_2k.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    tokenized_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "dde48c77-d16f-478a-9942-830486f1ce9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Agregamos EOS y BOS\n",
    "tokenized_text = ['<s> ' + sent + ' </s>' for sent in tokenized_text.split('\\n')]\n",
    "tokenized_text = '\\n'.join(tokenized_text).replace('.','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "610f93fe-4340-4509-a219-57dbc34ca48a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_corpus = tokenized_text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "53b5fea1-26c6-4e11-a9ce-af6512734dda",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: 574450\n",
      "types in corpus: 2062\n"
     ]
    }
   ],
   "source": [
    "print(\"tokens:\", len(tokenized_corpus))\n",
    "types_in_corpus = Counter(tokenized_corpus)\n",
    "print(\"types in corpus:\", len(types_in_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "97f11e1c-c1de-43d1-9488-2794ed40e7a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 39209),\n",
       " ('que', 20203),\n",
       " ('de', 18019),\n",
       " ('y', 17784),\n",
       " ('la', 10723),\n",
       " ('a', 10677),\n",
       " ('<s>', 9248),\n",
       " ('</s>', 9248),\n",
       " ('en', 8656),\n",
       " ('el', 8103),\n",
       " ('no', 6651),\n",
       " ('los', 5103),\n",
       " ('se', 5073),\n",
       " (';', 4709),\n",
       " ('con', 4059)]"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "types_in_corpus.most_common(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9842edfc-70f8-4af4-b168-f5fa1542b34e",
   "metadata": {},
   "source": [
    "Ahora sí, formamos el corpus con las oraciones tokenizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "63f33613-b612-48d3-8fb4-f2d3499a86aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_corpus_sentences = [ sent.split() for sent in tokenized_text.split('\\n') ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "608ae7c1-a6e7-4e70-8273-cf4235e0245d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'en', 'un', 'lugar', 'de', 'la', 'mancha', ',', 'de', 'cuyo', 'nombre', 'no', 'quiero', 'acor@@', 'darme', ',', 'no', 'ha', 'mucho', 'tiempo', 'que', 'vi@@', 'vía', 'un', 'hidalgo', 'de', 'los', 'de', 'lanza', 'en', 'as@@', 'ti@@', 'll@@', 'ero', ',', 'ad@@', 'ar@@', 'ga', 'anti@@', 'gua', ',', 'ro@@', 'c@@', 'ín', 'f@@', 'lac@@', 'o', 'y', 'gal@@', 'go', 'corre@@', 'dor', '</s>']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_corpus_sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16164c5-44d9-4a1b-a7e0-95fe986b614c",
   "metadata": {},
   "source": [
    "### Diccionario de vocabulario\n",
    "Nuestro modelo del lenguaje requiere que pasemos nuestras palabras a indices numericos. Utilizaremos enteros para estimar el modelo. \n",
    "\n",
    "Crearemos dos diccionarios: \n",
    "1. el primero tomara la palabra y lo convertira a indice (Para acceder a las probabilidades del modelo) \n",
    "2. El segundo tomará los indices y los convertira de vuelta a palabras (Nos ayudará a recuperar las palabras a partir de los índices del modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "d652a81a-0ac7-4863-8ee8-9737dcfcee8d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index of word 'lugar':   3\n",
      "word in index 3:         lugar\n"
     ]
    }
   ],
   "source": [
    "vocabulary = list(types_in_corpus.keys())\n",
    "vocabulary_by_type = { tipo: indice for (tipo, indice) in zip(vocabulary, range(len(vocabulary)))}\n",
    "vocabulary_by_index = { indice: tipo for (tipo, indice) in zip(vocabulary, range(len(vocabulary)))}\n",
    "\n",
    "print(\"index of word 'lugar':  \", vocabulary_by_type['lugar'])\n",
    "print(\"word in index 3:        \", vocabulary_by_index[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e09ec9d-f65b-4005-aa76-238b92ae8432",
   "metadata": {},
   "source": [
    "### Indexar tokens del corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "e974a37a-288a-49b2-9abf-9eb76ec08bf0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def word_to_index(corpus: list[list[str]], vocab_by_type) -> list[list[int]]:\n",
    "    \"\"\"Function that maps each word in a corpus to a unique index\"\"\"\n",
    "    return [ [vocab_by_type[token] for token in sent] for sent in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "ba0f9298-aa04-459e-b1b3-4ee91a272755",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "indexed_corpus_sentences = word_to_index(tokenized_corpus_sentences, vocabulary_by_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "b11f929b-8981-4bba-9053-194669892439",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 4, 8, 9, 10, 11, 12, 13, 7, 10, 14, 15, 16, 17, 18, 19, 2, 20, 4, 21, 4, 22, 1, 23, 24, 25, 26, 7, 27, 28, 29, 30, 31, 7, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43]\n"
     ]
    }
   ],
   "source": [
    "print(indexed_corpus_sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e594b969-147c-4c1b-bf78-7917c7a69ea1",
   "metadata": {},
   "source": [
    "## Dividir en conjuntos de entrenamiento y prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "9f7ee56a-cb43-47b7-92ee-d5e467cc3bd0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train len: 6473 test len: 2775\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "corpus_train, corpus_test = train_test_split(indexed_corpus_sentences, test_size=0.3)\n",
    "\n",
    "len(corpus_train) + len(corpus_test) == len(indexed_corpus_sentences)\n",
    "\n",
    "print(\"Train len:\", len(corpus_train), \"test len:\", len(corpus_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d490e3-a987-4cbe-8691-20cba58a89cd",
   "metadata": {},
   "source": [
    "## Estimación del modelo de n-gramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "e27b8e9f-3c18-47e2-a7a2-11945ecf2692",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import chain\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_n_grams(indexed_sents: list[list[str]], n=2) -> chain:\n",
    "    return chain(*[zip(*[sent[i:] for i in range(n)]) for sent in indexed_sents])\n",
    "\n",
    "def get_model(sents: list[list[str]], vocabulary: defaultdict, n: int=2, l: float=1.0) -> tuple:\n",
    "    BOS_IDX = vocabulary_by_type['<s>']\n",
    "    EOS_IDX = vocabulary_by_type['</s>']\n",
    "\n",
    "    # Get n_grams\n",
    "    n_grams = get_n_grams(sents, n)\n",
    "\n",
    "    # Get n_grams frequencies\n",
    "    freq_n_grams = Counter(n_grams)\n",
    "\n",
    "    # Get vocabulary length (WITH BOS/EOS)\n",
    "    N = len(vocabulary)\n",
    "    # Calculate tensor dimentions for transition probabilities\n",
    "    dim = (N,)*(n-1) + (N,)\n",
    "\n",
    "    # Transition tensor\n",
    "    A = np.zeros(dim)\n",
    "    # Initial Probabilities\n",
    "    Pi = np.zeros(N)\n",
    "\n",
    "    for n_gram, frec in freq_n_grams.items():\n",
    "        # Fill the tensor with frequencies\n",
    "        if n_gram[0] != BOS_IDX:\n",
    "            A[n_gram] = frec\n",
    "        # Getting initial frequencies\n",
    "        elif n_gram[0] == BOS_IDX and n_gram[1] != EOS_IDX:\n",
    "            Pi[n_gram[1]] = frec\n",
    "\n",
    "    # Calculating probabilities from frequencies\n",
    "    # We consider the parameter `l` for Lidstone Smoothing\n",
    "    for h, b in enumerate(A):\n",
    "        A[h] = ((b+l).T/(b+l).sum(n-2)).T\n",
    "\n",
    "    # Calculating initial probabilities\n",
    "    Pi = (Pi+l)/(Pi+l).sum(0)\n",
    "\n",
    "    # We get our model\n",
    "    return A, Pi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b866406-0821-4fb3-bc13-bd56e33b5a7d",
   "metadata": {},
   "source": [
    "Estimando un modelo de bigramas con  λ=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "8645a9b1-cee5-44c6-a7cb-28b0a8a20953",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cinda'"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_by_index[2061]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "13f27868-dace-442f-ad11-46444697a277",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "bigram_model = get_model(corpus_train, vocabulary_by_type, n=2, l=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "932ccee0-add1-4635-b7c3-88ebe97f8467",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor dimention (2062, 2062)\n",
      "Suma de probabilidades\n",
      "[1. 1. 1. ... 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "A_bigram = bigram_model[0]\n",
    "print(\"Tensor dimention\", A_bigram.shape)\n",
    "print(\"Suma de probabilidades\")\n",
    "print(A_bigram.sum(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11301ca-0416-4fdc-b7e9-210292015ca1",
   "metadata": {},
   "source": [
    "Ahora de trigramas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "dffea475-4f86-47af-a99e-adb4e0ebc6cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 65.3 GiB for an array with shape (2062, 2062, 2062) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:1\u001b[0m\n",
      "Cell \u001b[1;32mIn[327], line 24\u001b[0m, in \u001b[0;36mget_model\u001b[1;34m(sents, vocabulary, n, l)\u001b[0m\n\u001b[0;32m     21\u001b[0m dim \u001b[38;5;241m=\u001b[39m (N,)\u001b[38;5;241m*\u001b[39m(n\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m (N,)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Transition tensor\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m A \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Initial Probabilities\u001b[39;00m\n\u001b[0;32m     26\u001b[0m Pi \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(N)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 65.3 GiB for an array with shape (2062, 2062, 2062) and data type float64"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trigram_model = get_model(corpus_train, vocabulary_by_type, n=3, l=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272b8a8a-e193-44c7-bb0d-9589352eed61",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_trigram = trigram_model[0]\n",
    "print(\"Tensor dimention\", A_trigram.shape)\n",
    "print(\"Suma de probabilidades\")\n",
    "print(A_trigram.sum(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bccfda5-b6a2-45de-a06b-71db7e11ab16",
   "metadata": {},
   "source": [
    "## Aplicando el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "37f1ebd9-6c20-4d3a-a732-edf704faf4c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_sent_probability(indexed_sentence: str, vocab: defaultdict, model: tuple) -> float:\n",
    "    A, Pi = model\n",
    "    # Getting the n from n-grams\n",
    "    n = len(A.shape)\n",
    "    first_indexed_word = indexed_sentence[0]\n",
    "    # Getting initial probability\n",
    "    try:\n",
    "        probability = np.log(Pi[first_indexed_word])\n",
    "    except:\n",
    "        print(f\"[WARN] OOV for word as BOS with index={first_indexed_word}\")\n",
    "        probability = 0.0\n",
    "\n",
    "    # Getting n-grams of the sentence\n",
    "    n_grams = get_n_grams([indexed_sentence], n)\n",
    "    for n_gram in n_grams:\n",
    "        try:\n",
    "            probability += np.log(A[n_gram])\n",
    "        except:\n",
    "            print(f\"[WARN] OOV for n_gram={n_gram}\")\n",
    "            probability += 0.0\n",
    "\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "305ed229-bce0-4cc5-992f-1ae9328e1fe2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_next_word(indexed_sentence: list[int], vocab: defaultdict, model: tuple) -> str:\n",
    "    A, Pi = model\n",
    "    history = len(A.shape) - 1\n",
    "    prev_n_gram = tuple(indexed_sentence[-history:])\n",
    "    probability = get_sent_probability(indexed_sentence, vocab, model)\n",
    "    next_word = np.argmax(probability + np.log(A[prev_n_gram]))\n",
    "    return next_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "c821ab68-922f-4f28-a5d3-0a0b23ee72c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test sentence: ['<s>', 'lo', 'mismo', 'se', 'le', 'dijo', 'al', 'padre', 'de', 'zoraida', ',', 'el']\n"
     ]
    }
   ],
   "source": [
    "TEST_SENTENCE = corpus_test[0][:12]\n",
    "print(\"Test sentence:\", [vocabulary_by_index[i] for i in TEST_SENTENCE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "92e0c72f-b011-4a39-973a-9eac4f6c3872",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'que'"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_word_index = predict_next_word(TEST_SENTENCE, vocabulary_by_type, bigram_model)\n",
    "vocabulary_by_index[next_word_index]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
